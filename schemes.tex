%!TEX root = raspi_journal.tex
\label{sec:schemes}

In this section, we present the considered coding schemes that are evaluated
in the \ac{Raspi} 1 and 2.
%In this section, we present a description of the considered coding
%schemes measured with the \ac{Raspi} testbed.
We introduce a
definition for the primitive coding operations, e.g. encoding,
decoding and recoding (where it applies) for each coding
scheme. Later, we address particular schemes which are obtained by
modifying the basic coding operations that provide better processing
speeds, which is particularly relevant for the \ac{Raspi}.  Finally,
we include a review of algorithms for network coding that exploit
multicore capabilities of the \ac{Raspi} 2.


\subsection{Random Linear Network Coding}
\label{ssec:RLNC}

\ac{RLNC} is an example of intra-session \ac{NC}, i.e., data symbols
from a single flow are combined with each other. In this type of
network coding, all the original data packets $P_j, j \in [1,g]$, each
of $B$ bytes, are used to create coded packets using random linear
combinations of the original ones. In the following subsections, we
describe the basic functionalities of \ac{RLNC}.

\subsubsection{Encoding}
In \ac{RLNC}, any coded packet is a linear combination of all
the original packets. For the coding scheme, packets are seen as
algebraic entities formed as a sequence of elements from $GF(q)$,
which is a \ac{GF} of size $q$. Later, each original packet is
multiplied by a coding coefficient from $GF(q)$. The coding coefficients
are chosen uniformly at random from the \ac{GF} by the encoder. To
perform the multiplication of a packet by a coding coefficient, the
coefficient is multiplied for each of the elements in the
concatenation that composes an original packet, preserving the
concatenation. Later, all resulting packets are added within the
\ac{GF} arithmetics together to generate a coded packet. Thus, a
coded packet can be written as:
%
\begin{align} \label{eq:coded_packet}
C_i  &= \bigoplus_{j=1}^{g} v_{ij} \otimes P_j ,\ \forall i \in [1,\ldots)
\end{align}

In \eqref{eq:coded_packet}, $C_i$ is the generic coded packet. In principle,
the encoder may produce any number of coded packets, but a finite
number is produced in practice given that a decoder needs only
$g$ linearly independent coded packets to decode the batch. Also, in
\eqref{eq:coded_packet}, $v_{ij}$ is the coding coefficient used in the
$i$-th coded packet and assigned to multiply the $j$-th original packet.

For indicating to a receiver how the packets were combined
to create a coded one, a simple yet versatile choice is to append
its coding coefficients as a header in the coded packet. Hence, an
amount of overhead is included in every coded packet given that we
need to provide some signaling needed for decoding. The coding
coefficients overhead amount for packet $i$, $|v_i|$, can be quantified
as:
%
\begin{align} \label{eq:coded_packet_coef}
|v_i| &= \sum_{j=1}^{g} |v_{ij}| = g \times \lceil \log_{2} (q)
\rceil ~ [\mathrm{bits}].
\end{align}

\subsubsection{Decoding}
\label{sssec:decoding}
To be able to decode a batch of $g$ packets, a linearly independent set of $g$
coded packets, $C_i,\ i \in [1,g]$ is required at a decoder.
Once this set has been collected for a decoder,
the original packets can be found by computing the solution of a system
of linear equations using \ac{GF} aritmethics. Thus, we define
$\textbf{C} = \left[C_1 \ldots C_g \right]^T$,
$\textbf{P} = \left[P_1 \ldots P_g \right]^T$ and the coding matrix
$\textbf{V}$ that collects the coding coefficients for each of the $g$
coded packets, as follows:
%
\begin{align} \label{eq:coding_matrix}
\textbf{V} =
\left[
\begin{array}{c}
        v_1    \\ \hline
        \vdots \\ \hline
        v_g    \\
\end{array}
\right]
=
\left[
\begin{array}{ccc}
        v_{11} & \ldots & v_{1g} \\
        \vdots  & \ddots & \vdots  \\
        v_{g1} & \ldots & v_{gg} \\
\end{array}
\right].
\end{align}
%
Algebraically, decoding simplifies to finding the inverse of
$\textbf{V}$ in the linear system
$\textbf{C} = \textbf{V} \textbf{P}$, which can be achieved using
efficient Gaussian elimination techniques \cite{fragouli2006network}.
On real applications, decoding is performed on-the-fly, e.g., the
pivots are computed as packets are progressively received, in order to
minimize the computation delay for each step.

A decoder starts to calculate and substract contributions from each
of the pivot elements, e.g. leftmost elements in the main diagonal of
\eqref{eq:coding_matrix}, from top to bottom. The purpose is to obtain
the equivalence  $\textbf{V}$ in its reduced echelon form. The steps
for reducing the matrix by elementary row operations are carried out each time
a linearly independent packet is received. Once in reduced echelon form, packets
can be retrieved by doing a substitution starting from the latest
coded packet. In this way, the amount of elementary operations at the end
of the decoding process is diminished.

\subsubsection{Recoding}
As an inherent property of \ac{RLNC}, an intermediate node in the network
is able to create new coded packets without needing to decode previously
received packets from an encoding source. Therefore, \ac{RLNC} is an end-to-end
coding scheme that permits to \textit{recode} former coded packets at any
point in the network without requiring a local decoding of the data. In
principle, a recoded packet should be indistinguishable from a coded one.
Thus, we define a recoded packet as $R_i$ and consider the coding
coefficients $w_{i1}, \ldots, w_{ig}$ as used to
create $R_i$. Later, a recoded packet can be written as:
%
\begin{align}
\label{eq:recoded_packet}
R_i = \bigoplus_{j=1}^{g} w_{ij} \otimes C_j  ,\ \forall i \in [1,\ldots).
\end{align}

In \eqref{eq:recoded_packet}, $w_{ij}$ is the coding coefficient used
in the $i$-th recoded packet and assigned to multiply a previously
coded packet $C_j$. These coding coefficients are again uniformly and
randomly chosen from $GF(q)$. However, these $w_{ij}$'s are not
appended to the previous coding coefficients.  Instead, the system
will update the previous coefficients. Due to the linearity of the
operation, this update reduces to recombining the coding coefficients
equivalent to each original packet with the weight of the
$w_{i,j}$.Therefore, we define the local coding matrix $\textbf{W}$ in
the same way as it was made for $\textbf{V}$. Thus, the local coding
matrix can be written as:
%
\begin{align} \label{eq:local_coding_matrix}
\textbf{W} =
\left[
\begin{array}{c}
        w_1    \\ \hline
        \vdots \\ \hline
        w_g    \\
\end{array}
\right]
=
\left[
\begin{array}{ccc}
        w_{11} & \ldots & w_{1g} \\
        \vdots  & \ddots & \vdots  \\
        w_{g1} & \ldots & w_{gg} \\
\end{array}
\right].
\end{align}
%
With the definitions from \eqref{eq:coding_matrix} and
\eqref{eq:local_coding_matrix}, the recoded packets
$\textbf{R} = \left[R_1 \ldots R_g \right]^T$ are written as
$\textbf{R} = (\textbf{W} \textbf{V}) \textbf{P}$. Here, we recognize
the relationship between original and recoded packets. The resulting
coding matrix is the multiplication of matrices $\textbf{W}$ and
$\textbf{V}$. Denoting ${(\textbf{W} \textbf{V})}_{ij}$ as the element
in the $i$-th row and $j$-th column of $(\textbf{W} \textbf{V})$, this
term is the resulting coding coefficient used to create $R_i$ after
encoding the original packet $P_j$ recoding it locally in an intermediate
node. Finally, the appended coefficients for $R_i$ are
${(\textbf{W} \textbf{V})}_{ik}$ with $k \in [1,g]$. By doing some
algebra, each ${(\textbf{W} \textbf{V})}_{ij}$ term can be verified to be
computed as:
%
\begin{align}
\label{eq:appended_coded_coefficients}
{(\textbf{W} \textbf{V})}_{ij} = \sum_{k=1}^{g} w_{ik} v_{kj}\ ,\ \forall i,j \in [1,g] \times [1,g].
\end{align}
%
This update procedure on the coding coefficients is carried by all the
recoders in a given network therefore allowing any decoder to compute
the original data after Gaussian elimination, regardless of the amount
of times recoding was performed and without incurring in any
additional overhead cost for signaling. Similar to the encoding
operation, any decoder that collects a set of $g$ linearly independent recoded
packets with their respective coefficients, will be able to decode the
data as mentioned before in Section \ref{sssec:decoding}.

\subsection{Sparse Random Linear Network Coding}

In \ac{SRLNC}, instead of considering all the packets to create a
coded packet as in \ac{RLNC}, an encoder sets more coding coefficients
to zero when generating a coded packet with the purpose of reducing
the overall processing.  Decoding is the same as in \ac{RLNC}, but
given that the coding matrices are now sparse means that there will be less
operations to perform in the decoding process.  Recoding, although
theoretically possible, is omitted since it requires the use of
heuristics to keep packets sparse after recoding, which is inherently
sub-optimal.
%In what follows, we describe the coding scheme with a
%basic method to produce sparse coded packets and later we indicate a
%second one, giving reference to their implementation performance.
In what follows, we describe the coding scheme with two
different methods to produce sparse coded packets.

\subsubsection{Method 1: Fixing the Coding Density}
A way to control the amount of non-zero coding coefficients, is to set a
fixed ratio of non-zero coefficients in the encoding vector of size $g$. We
refer to this fixed ratio as the average coding density $d$. Thus, for any
coded packet $C_i$ with coding coefficients $v_{ij},\ j \in [1,g]$, its
coding density is defined as follows:
%
\begin{align}
\label{eq:coding_density}
d = \frac{\sum_{j=1}^{g} f(v_{ij})}{g},\ \
    f(v_{ij}) =
    \begin{cases}
        0 &,\ v_{ij} = 0 \\
        1 &,\ v_{ij} \neq 0
    \end{cases}
\end{align}
%
From the density definition in~\eqref{eq:coding_density}, it can be
observed that $0 \leq d \leq 1$. As $g$ increased, we obtain more
granularity in the density. Notice that the special case of  $d = 0$ has no
practical purpose since it implies just zero padded data.
\textcolor{red}{exclude the case d=1 for gf(2) here}
So, a practical
range for the density excludes the zero case, $0 < d \leq 1$.
%Moreover,
%it can be also observed that the case of $d = 1$ reduces to the \ac{RLNC}
%coding scheme described in Section~\ref{ssec:RLNC}.
Moreover, the densities $d=0.5$ in $GF(2)$ and $d=1$ in higher order
\ac{GF}s is similar to the \ac{RLNC} scheme described in Section~\ref{ssec:RLNC}.

To achieve a desired average coding density in the coefficients, for each
of them, we utilize a set of Bernoulli random variables all with parameter
$d$ as its success probability, i.e. $\mathbb{B}_j \sim Bernoulli(d)\ ,\ \forall
j \in [1,g]$. In this way, we can represent a coded packet in \ac{SRLNC} as:
%
\begin{align} \label{eq:sparse_coded_packet}
    C_i  &= \bigoplus_{j=1}^{g} \mathbb{B}_j v_{ij} \otimes P_j ,\ v_{ij}
    \neq 0\
    \forall i \in [1,\ldots) ,\ \
    d \in
    \begin{cases}
        {(0,0.5]} & ,\ q = 2 \\
        {(0,1]}   & ,\ q > 2
    \end{cases}
\end{align}
%
In~\eqref{eq:sparse_coded_packet}, we have the requirement for the
coding coefficient to not be zero, since we want to ensure a coding
coefficient is generated for any random trial, where
$\mathbb{B}_j = 1$. Therefore, in our implementation of \ac{SRLNC} we
exclude the zero element and then pick uniformly distributed random
elements from $GF(q)-\{ 0 \}$. Also, we have specified a dependency on
the field size for practical density values. In case of employing
$GF(2)$, the maximum plausible density is restricted up to 0.5 since
higher values incur in more frequent linearly dependent coded
packets~\cite{realworld_nc2013}.\textcolor{red}{Rather more coding complexity}

Reducing $d$ enables the encoder to decrease the average number of
packets mixed to make a coded one. This reduces the complexity
of the encoder since it needs to mix less packets. Moreover, it also
simplifies the decoder processing given that less nonzero coding
coefficients are required to be operated during the Gaussian
elimination stage.

The drawback of this scheme is that coded packets from the encoder
become more linearly dependent on each other as the density is reduced.  This
leads to transmission overhead since another coded packet is required
to be sent for every reception of a redundant packet.  Also, this
method may still generate a coded packet, which does not contain any
information. For example, we might find the case where
$\left[\mathbb{B}_1, \ldots, \mathbb{B}_g \right] = \mathbf{0}$ may
happen frequently for low densities. In that case, the encoder
discards the coded packet and tries to generate a new one to avoid the
negative impact on overall system performance.

\subsubsection{Method 2: Sampling the Amount of Packets to Combine}
The method described from \eqref{eq:sparse_coded_packet} results
in a fast implementation in terms of execution time for
$d \geq 0.3$~\cite{practicalview_tsnc2015}.
It is however not able to utilize the full performance potential for
low densities as the total number of Bernoulli trials remain unchanged
independently of the density.
%Nevertheless, for $d < 0.3$
%the implementation becomes slow given that more Bernoulli trials are
%required in total for all random variables to generate a coded packet.
Thus, we introduce a second method that permits a faster implementation
for low coding densities~\cite{practicalview_tsnc2015}.

For this method, we first obtain the amount of packets
that we will combine to create a coded packet. To do so, a random
number, $\mathbb{N}$, of the original packets is used to produce a
coded packet. For our case, $\mathbb{N}$ is binomially distributed
with parameters $g$ for the number of trials and $d$ for its success
probability, e.g. $\mathbb{N} \sim Binomial(g,d)$. However,
when sampling from this distribution, the case
$\mathbb{N} = 0$ occurs with a non-zero probability. In order to handle
this special case, our implementation considers
$\mathbb{K} = \max(1,\mathbb{N})$ as the final amount of packets to be
mixed together. In this way, we always ensure that at least one packet is
encoded.

The only caveat is that the case of $\mathbb{K} = 1$ occurs slightly
more often than $\mathbb{N} = 1$ in the original distribution, but for
the considered density range in this method, this is not a significant
modification~\cite{practicalview_tsnc2015}. Then, once the
distribution for the number of packets to mix has been defined, we
sample a value $n$ from $\mathbb{N}$ and compute $k =
\max(1,n)$. Later, we create a set $\mathcal{K}$ with
cardinality $k$, e.g. $|\mathcal{K}| = k$, where the elements in
$\mathcal{K}$ are the indexes of the packets that are going to be
considered for making a coded packet. To compute the indexes
of the set $\mathcal{K}$, we do the following algorithm in pseudo-code: \\
\ \\
\begin{algorithm}[H]
 \label{alg:k_set}
 \KwData{$k$: Size of $\mathcal{K}$. $g$: Generation Size}
 \KwResult{$\mathcal{K}$: The set of non-repeated indexes}
 $\mathcal{K}$ = \{ \}\;
 \While{$|\mathcal{K}| \neq k$ }{
  $i$ = Sample from $\mathbb{U}(1,g)$\;
  \If{$i \notin \mathcal{K}$}{
   Insert $i$ in $\mathcal{K}$\;
   }
 }
 \caption{Computation of the set of indexes for packet combination in SRLNC.}
\end{algorithm}
\ \\
In Algorithm~\ref{alg:k_set}, the notation $\mathbb{U}(1,g)$ stands
for a uniform discrete random variable with limits $1$ and $g$. The
pseudo-code in Algorithm~\ref{alg:k_set} indicates that the set $\mathbb{K}$
is filled with non-repeated elements taken uniformly at random from
$[1,\ldots,g]$. Finally, once the set of indexes has been defined, a coded
packet using \ac{SRLNC} can be written as shown in
\eqref{eq:sparse_coded_packet2}:
%
\begin{align} \label{eq:sparse_coded_packet2}
    C_i  &= \bigoplus_{m \in \mathcal{K}} v_{im} \otimes P_{m},\ v_{im} \neq 0,\ \forall i \in [1,\ldots)
\end{align}

\subsection{Tunable Sparse Network Coding}

\ac{TSNC} can be considered an extension to \ac{SRLNC}. The key idea is
not only for the encoder to generate sparse coded packets, but also to
modify the code density of the packets progressively as required. As a
decoder accomulates more coded packets, the probability that the next
received coded packet will be linearly dependent increases~\cite{Feizi2012}.
%\cite{feizi2014tunable}.

Therefore, in \ac{TSNC}, a simple procedure for controlling this probability
is to gradually increase the coding density as the \ac{dof}\footnote{We refer
as degrees of freedom to the dimension of the linear span from the coded
packets that a decoder has at a given point} increases, e.g., as the
cardinality of the set of linearly independent coded packets increases. This enables
\ac{TSNC} to significantly reduce the complexity, particularly in the
beginning of the transmission process and also to control the decoding
delay. For \ac{TSNC}, we define the budget, $b \geq g$, to be a target
number of coded packets a transmitter wants to send to a receiver
for decoding a generation of $g$ packets. In some scenarios, we may set
$e$ defined as the excess of linearly dependent packets sent from the encoder.
This helps to also define the budget as $b = g + e$.

The difference between $b$ and $g$ is equal to the losses in the channel
and the losses due to linear dependencies. Therefore, in a lossless
channel, the budget is:
\begin{equation}\label{eq:budget}
    b(g,d)=\sum^{g-1}_{i=0}\frac{1}{P(i,g,d)},
\end{equation}
where $P(i,g,d)$ is the probability of receiving an innovative coded packet
after receiving $i$ linearly independent packets with a coding density $d$. In our
implementations, we considered the lower bound for the innovation probability
from \cite{feizi2014tunable}, given as:
\begin{equation}
    P(i,g,d) \geq 1-(1-d)^{g-i}
\end{equation}

Provided a desired budget $b$ and the \ac{dof} of the decoder, an encoder
can estimate the required coding density for the packets to not exceed the
budget. In our implementation, we use feedback packets to provide the
encoder an estimate of the decoder \ac{dof} at pre-defined points of the
transmission process. The points in the process occur when a decoder
obtains a given amount of \ac{dof}. In our implementation, we define $r(k)$ as the \ac{dof}s of the decoder where it has to send the $k$-th feedback in
\eqref{eq:feedback_dofs}.
%
\begin{equation}\label{eq:feedback_dofs}
    r(k) = \floor{g\cdot \left[\frac{2^k -1}{2^k}\right]},\ k \in \Big[1,\ldots,\ceil{log_2(g)}+1\Big]
\end{equation}
%
At the beginning of the transmission process, we assume that the encoder
starts with an initial coding density that has been calculated depending to
the budget. According to \eqref{eq:feedback_dofs}, a decoder sends feedback
once it has obtained (rounded down) $g/2,\ 3g/4,\ 7g/8,\ \ldots$  degrees of
freedom. The total amount of feedback for this scheme will depend on the
generation size. Still, we have an implementation that limits the amount of
feedback to be sent from the decoder in~\cite{practicalview_tsnc2015}.

The transmissions made between feedback packets are called the density
regions since the same estimated density is used for encoding sparse
packets. Once a new feedback is received, an encoder recalculates the
coding density for the new region until the next feedback packet is
received. Before the coding density can be estimated, it is essential
that the encoder has a good estimate of the decoder's \ac{dof}, but also
the remainder of the total budget. To calculate the density, we use
bisection to estimate a fixed density for the density region that
satisfies the budget for the $k$-th density region:
\begin{equation}
b(r(k),r(k+1),g,d) = \sum^{r(k+1)}_{i=r(k)} \frac{1}{P(i,g,d)} = \sum^{r(k+1)}_{i=r(k)} \frac{1}{1-(1-d)^{g-i}} = \frac{b}{2^k},\ k\in [1,\ldots)
\end{equation}
%
% Where the budget is half of the remaining budget for all the density regions
% except the last one, to which it is assigned the remaining of the budget.
The feedback scheme based on the rank reports
in~\eqref{eq:feedback_dofs} roughly splits the remaining coding
regions in two halves. In other words, the first half is the region
where the encoder currently operates. Once an encoder finishes with
this region, it proceeds with second half where again it splits this
into two new regions and so on, until it finishes the transmission the
whole generation. This is also the case for the budget that is split
equally among the two new regions. The very last region will be
assigned the full remainder of the total budget and the coding density
will not vary.

\subsection{Network Coding Implementation for the Raspberry Pi Multicore
Architecture}
\label{sub:implementation-multicore}

The arithmetic operations needed to encode and decode data are, in
general, similar. To encode packets, the encoder needs to perform the
matrix multiplication $\textbf{C} = \textbf{V} \textbf{P}$. On the
other hand, decoding the information requires the decoder to find
$\textbf{V}^{-1}$ and to perform the matrix multiplication
$\textbf{P} = \textbf{V}^{-1} \textbf{C}$. In both cases, a matrix
multiplication is needed. Therefore, to make a practical
implementation of network coding, it is valuable to find a way to
optimize the matrix multiplications operations for multicore
architectures.

When designing multi-threaded algorithms for network coding
operations, it is possible to implement the decoding by combining the
matrix inversion and the matrix multiplication, e.g., performing the
Gauss-Jordan algorithm over the coding matrix $\textbf{V}$ while
performing, in parallel, row operations on the coded data
$\textbf{C}$. For example, in~\cite{5061951} and~\cite{4262451} the
parallelization of the row operations are optimized for \ac{GPU} and
\ac{SMP} systems respectively. However, the parallelization of such
operations provide limited speeds up for small block sizes
($\leq 2048$ bytes). The reason is that to operate in a parallel
fashion over the same coded packet $C_i$ requires strained
synchronization.

Therefore, to overcome the constraints of tight synchronization, a
preferable option is to explicitly invert the matrix $\textbf{V}$, and then
take advantage of optimizations for matrix multiplications, both at
encoding and decoding time. With that purpose, authors
in~\cite{wunderlich2015network} implemented an algorithm that adopts
the ideas of efficient \ac{BLAS}~\cite{lawson1979basic} operations
reimplementing them for finite fields operations. Although there are
libraries such as \cite{dumas2008dense} and \cite{dumas2002linbox} that allow
highly optimized finite field \ac{BLAS} implementations, they work on
converting the \ac{GF} elements into floating point numbers and back.
Even though the approach is efficient for large matrix sizes, the
numerical type conversion overhead is not suitable for matrix sizes of
network coding implementations.

The implemented algorithm in~\cite{wunderlich2015network} aims to be
cache-efficient by maximizing the number of operations performed over a
fetched data block. Here, the matrices are divided in square sub-blocks
where we can operate each of them. As a consequence, this technique
exploits the spatial locality of the data, at least for
$\mathcal{O}(n^3)$ algorithms~\cite{golub2012matrix}. The optimal block
size is architecture-dependent. The ideal block has a number of operands
that fit into the system L1 cache, and it is a multiple of the \ac{SIMD}
operations size.

The idea of the implemented algorithm is to represent each one of the sub-block
matrix operations: matrix-matrix multiplication, matrix-triangle matrix
multiplication, triangle-matrix system solving, etc; as a base or
\textit{kernel} operation that can be optimized individually using \ac{SIMD}
operations. Each kernel operation, at the same time, can be represented as a
task with inputs and outputs in memory that can be assigned to the cores as soon
as the dependencies are satisfied. The benefit of this method is that the
synchronization relies only on data dependencies and it does not require the
insertion of artificial synchronization points. Using this technique, the matrix
inversion is performed using an algorithm based on LU factorization
\cite{Dongarra:2011:HPM:2132876.2132885}, and the matrix multiplication is
performed by making the various matrix-matrix multiplications on the sub-blocks.
