%!TEX root = raspi_journal.tex
\label{sec:measurements}

%With the testbed, methodology and setups from the previous sections, we
With the methodology and setups from the previous sections, we
proceed to obtain the measurements for the \ac{Raspi} 1 and 2 devices.
We consider the following set of parameters for our study:
For all the codes, we use $g = [16,\ 32,\ 64,\ 128,\ 256,\ 512]$
and $q = [2,\ 2^8]$. For the single-core implementations
and cases when the generation size is varied $B = 1600$ bytes.
We consider another setup where only the packet size varies,
$B = [64,\ 128,\ 256,\ 512,\ 1024,\ 2048,\ 4096,\ 8192,\ 16384,\ 32768
,\ 65536,\ 131072]$ bytes with a generation size fixed on $g = [16,\ 128]$
to see the performance of the \ac{Raspi}s in low and high
packet size regimes. The third setup we considered used $B = 1536$ bytes for the optimized
multicore implementation. 
%For \ac{SRLNC}, we considered $d = [0.02,\ 0.1]$.
\ac{SRLNC} was measured with the densities $d = [0.02,\ 0.1]$.
For \ac{TSNC}, we considered excess packets,
$e = [8,\ 16]$, so that the budget is $b = g + e$. In all our
measurement reports, to simplify their review, we first present the
results for the \ac{Raspi} 1 and later continue with the \ac{Raspi} 2.

\subsection{Goodput}
For the goodput, we separate the results according to their time benchmarks
as we did in Section \ref{sec:metrics}.
%since their consider different methodologies.
We proceed first with the measurements related to the encoder
and later review the case of the decoder.

\subsubsection{Encoding}
Fig. \ref{fig:enc_good_rasp1} shows the results of the encoder goodput
measurements for the \ac{Raspi} 1. Fig. \ref{fig:enc_good_rasp1_gen_gf2}
and \ref{fig:enc_good_rasp1_gen_gf256} present goodput as function of the
generation size for $GF(2)$ and $GF(2^8)$ respectively for a packet size
of $B = 1600$ bytes. Fig. \ref{fig:enc_good_rasp1_packet_gf2} and
\ref{fig:enc_good_rasp1_packet_gf256} present the same metric but now as
function of the packet size for $GF(2)$ and $GF(2^8)$ respectively. In this
case the generation size was set to $g = 16$ packets.

Similarly, Fig. \ref{fig:enc_good_rasp2} shows the same set of
measurements for the \ac{Raspi} 2. Hence, Fig.
\ref{fig:enc_good_rasp2_gen_gf2} and \ref{fig:enc_good_rasp2_gen_gf256}
present goodput as function of the generation size  and
Fig. \ref{fig:enc_good_rasp2_packet_gf2} and
\ref{fig:enc_good_rasp2_packet_gf256} as a function of the packet size
for the same cases mentioned previously. Therefore, we will proceed
to make the results analysis with \ac{Raspi} 1 and indicate which
similarities or differences occur for the \ac{Raspi} 2 and when do
they occur.
%
\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=1.15\textwidth]{images/06_06_2016/goodput_vs_generation_size_Rasp_encoder_Binary_1600.eps}
        \caption[]%
        {{\small Goodput vs. Generation size for $q = 2$}}
        \label{fig:enc_good_rasp1_gen_gf2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=1.15\textwidth]{images/06_06_2016/goodput_vs_generation_size_Rasp_encoder_Binary8_1600.eps}
        \caption[]%
        {{\small Goodput vs. Generation size for $q = 2^8$}}
        \label{fig:enc_good_rasp1_gen_gf256}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=1.15\textwidth]{images/06_06_2016/goodput_vs_symbol_size_Rasp_encoder_Binary_16.eps}
        \caption[]%
        {{\small Goodput vs. Packet size for $q = 2,\ g=16$}}
        \label{fig:enc_good_rasp1_packet_gf2}
    \end{subfigure}
    \quad
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=1.15\textwidth]{images/06_06_2016/goodput_vs_symbol_size_Rasp_encoder_Binary8_16.eps}
        \caption[]%
        {{\small Goodput vs. Packet size for $q = 2^8,\ g=16$}}
        \label{fig:enc_good_rasp1_packet_gf256}
    \end{subfigure}
    \caption[]
    {\small Encoder goodput measurements for the \ac{Raspi} 1}
    \label{fig:enc_good_rasp1}
\end{figure*}
%
As it can seen from Fig. \ref{fig:enc_good_rasp1_gen_gf2},
\ref{fig:enc_good_rasp1_gen_gf256}, \ref{fig:enc_good_rasp2_gen_gf2} and
\ref{fig:enc_good_rasp2_gen_gf256}; which indicate the goodput dependency
on the generation size, the encoding goodput gets reduced as the generation
size increases, regardless of the \ac{Raspi} model observed. The reason
is that the encoding operation processing is $\mathcal{O}(g)$ because
it is required to create $g$ coding coefficients and do $g$ multiplications
for each packet. This makes the goodput scale as the inverse of the
generation size.

In the sets of figures shown in Fig.~\ref{fig:enc_good_rasp1} and
\ref{fig:enc_good_rasp2}, it can be observed that the goodput for $GF(2)$
is higher than for $GF(2^8)$ but still around the same order of magnitude.
This difference is explained by noticing that \ac{GF} arithmetics in the
binary field are simply XOR or AND operations. These operations are
implemented efficiently in nowadays architectures.
%and are tailored to be fast in principle.
However, the operations in $GF(2^8)$
are more complex given that the finite field arithmetics have to be
performed with lookup tables which at the end reduces the computing
speed giving a lower goodput when compared with the binary field.

In Fig.~\ref{fig:enc_good_rasp1_gen_gf2}, \ref{fig:enc_good_rasp1_gen_gf256},
\ref{fig:enc_good_rasp2_gen_gf2} and \ref{fig:enc_good_rasp2_gen_gf256};
it can be seen that the goodput trends of five codes as a function of
the generation size: \ac{RLNC}, \ac{SRLNC} with $d = [0.02,\ 0.1]$
and \ac{TSNC} with an extra parameter that we mention. We define
$C$ as the number of density regions in the \ac{TSNC} transmission
process. The maximum number of possible regions depends
on the generation size~\eqref{eq:feedback_dofs}.
The larger the generation, the more density regions may
be formed and more density changes are possible.
Throughout this paper, \ac{TSNC} is configured to use the maximum number
of density regions possible. 
As there can be at least $C=1$ density region in a transmission, we use
$C=0$ to indicate the maximum possible density regions. This is
used for \ac{TSNC} in plots where the generation size is not fixed.
%In fact, \ac{TSNC} is configured to use the
%maximum possible density regions in all plots throughout this paper.
%The larger the generation, the more regions are formed and
%more density changes are possible.
%Throughout the section, we evaluate \ac{TSNC} with as many possible
%density regions that are possible to form during a transmission. This number
%grows as the generation increases, so we define $C=0$ as the maximum
%number of possible regions.
%
%We first evaluate \ac{TSNC} with no
%changes, e.g. $C = 0$ and the two amounts for dependent packets mentioned
%earlier. This means that we tolerate at most 8 or 16\fxnote{dependent packets} packets regardless of
%the generation. The goal with setting first \ac{TSNC} with these values
%is to observe first the sole effect of the budget in the code since
%$C = 0$ implies no density changes during the whole process.
%
For the plots in the mentioned figures from the encoder goodput measurements,
\ac{RLNC} presents the lowest performance in terms of goodput and
\ac{TSNC} with $e = 16$, the highest regardless of the \ac{Raspi} model.
Given that the processing time depends on the amount of coded packets
required to create, \ac{RLNC} is the slowest to process since it must
use all of the $g$ original packets.
%\fxnote{The encoding process speed depends on the total number of original packets to mix. Thus coding density and number of coded packets per generation}.
Later, sparse codes process the data
at a larger rate since less packets are being mixed when creating a
coded packet. The caveat of these schemes is that the sparser the code,
the more probable the ocurrences of linearly dependent packets is. So, basically
the sparser the codes, then more overhead due to transmissions of linearly dependent
packets.
%since it is more frequent to generate similar set of packets.
Excluding the coding coefficients overhead, the overhead due to
transmissions of linearly dependent packets might be high for the sparse schemes.
For example, if we consider \ac{TSNC} with $e = 16$ and $g = 16$
in Fig.~\ref{fig:enc_good_rasp1_gen_gf2}, the budget in this case permits
to send up to 32 packets which is $2\times$ the generation
size for an overhead of $100\%$ excluding the overhead from appending
the coding coefficients. This happens because \ac{TSNC}
has been allowed to add too much redundancy in this case.
%Given that $C=0$, this permits it to use a single very low code density $d$.
For \ac{RLNC},
this is not the case since the occurrence of linearly dependent coded packets is
small, because all coding coefficients are used. Even for $GF(2)$, the
average amount of redundant packets for \ac{RLNC} has proven to be 1.6
packets after $g$ have been transmitted \cite{trullols2011exact,zhao2012notes},
but less than the cases where sparse codes are utilized. Overall, we
observe that there is a trade-off between goodput and linearly dependent coded
packets transmission overhead.

For Fig.~\ref{fig:enc_good_rasp1_packet_gf2},
\ref{fig:enc_good_rasp1_packet_gf256}, \ref{fig:enc_good_rasp2_packet_gf2}
and \ref{fig:enc_good_rasp2_packet_gf256}; we see the packet
size effect in the encoding goodput in both \ac{Raspi} models and for
a fixed generation size. For \ac{TSNC} in this case, we allow for five density
changes during the generation transmission and again consider the same
budgets as before. In all the cases, we observe that there is an
optimal packet size to operate with. When reviewing the specifications
of the \ac{Raspi} 1, it uses a L1 cache of 16 KiB. Hence, the trends
in the packet size can be explained in the following way: For low packet
sizes, the data processing does not overload the \ac{CPU} of the
\ac{Raspi}, so the goodput progressively increases given that we
process more data as the packet size increases. However, after a certain
packet size, the goodput gets affected since the cache starts to saturate.
The packets towards the cache needs to have more processing. Beyond this
critical packet size, the \ac{CPU} just queues processing which incurs in
larger delay reducing the goodput.

If we consider the previous effect when reviewing the trends in the
mentioned figures, in all the models, we observe that the maximum coding performance is not
at 16 KiB but at a smaller value depending on the model, field size
and coding scheme considered. The reason is that the \ac{CPU} needs to
allocate computational resources to other processes from the different tasks
running in the \ac{Raspi}. Then, given there are various
tasks from other applications for proper functioning running in the
\ac{Raspi} at the same time as coding, the cache space is filled also
with the data from these tasks, thus diminishing the cache space available
for coding operations.

For the \ac{Raspi} 1 model, we observe in
Fig.~\ref{fig:enc_good_rasp1_packet_gf2} that the critical
packet size for \ac{RLNC} using $GF(2)$ occurs at 1 KiB whereas for the
sparse codes is close to 8 KiB in most cases for $GF(2)$. This
difference takes place since the sparse codes mix less packets
than \ac{RLNC}, which turns into less data loading in the
cache for doing computations. For a density of $d = 0.1$, packet size of
$B = 8$ KiB and $g = 16$ packets, we observe that roughly $\ceil{gd} = 2$
packets are inserted in the cache when calculating a coded packet with this
sparse code. Loading this into the cache, stands for 16 KiB which is the
cache size. A similar effect occurs for the other sparse codes. However, for
\ac{RLNC} given that it is a dense code since $d \to 1$, \ac{RLNC}
packets load data from all the $g = 16$ coding coefficients, which accounts
for the 16 KiB of the cache size. In
Fig.~\ref{fig:enc_good_rasp1_packet_gf256}, although the ideal packet
size remains the same for \ac{RLNC} and the sparse codes, the final goodput
is lower due to the field size effect.
%
\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=1.15\textwidth]{images/06_06_2016/goodput_vs_generation_size_Rasp_v2_encoder_Binary_1600.eps}
        \caption[]%
        {{\small Goodput vs. Generation size for $q = 2$}}
        \label{fig:enc_good_rasp2_gen_gf2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=1.15\textwidth]{images/06_06_2016/goodput_vs_generation_size_Rasp_v2_encoder_Binary8_1600.eps}
        \caption[]%
        {{\small Goodput vs. Generation size for $q = 2^8$}}
        \label{fig:enc_good_rasp2_gen_gf256}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=1.15\textwidth]{images/06_06_2016/goodput_vs_symbol_size_Rasp_v2_encoder_Binary_16.eps}
        \caption[]%
        {{\small Goodput vs. Packet size for $q = 2,\ g=16$}}
        \label{fig:enc_good_rasp2_packet_gf2}
    \end{subfigure}
    \quad
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=1.15\textwidth]{images/06_06_2016/goodput_vs_symbol_size_Rasp_v2_encoder_Binary8_16.eps}
        \caption[]%
        {{\small Goodput vs. Packet size for $q = 2^8,\ g=16$}}
        \label{fig:enc_good_rasp2_packet_gf256}
    \end{subfigure}
    \caption[]
    {\small Encoder goodput measurements for the \ac{Raspi} 2}
    \label{fig:enc_good_rasp2}
\end{figure*}
%
\par
The effects mentioned for the \ac{Raspi} 1 were also observed for
the \ac{Raspi} 2 as mentioned previously. Still, the \ac{Raspi} 2 achieves
roughly $5\times$ to $7\times$ gains in terms of encoding speed when comparing
the goodputs in Fig.~\ref{fig:enc_good_rasp1} and \ref{fig:enc_good_rasp2},
given that it has an \ac{ARM} Cortex A7 (v7) \ac{CPU} and twice the \ac{RAM}
size than the \ac{ARM}1176JZF-S (v6) core of the \ac{Raspi} 1 model.

In Fig.~\ref{fig:enc_good_rasp2_packet_gf2} and
\ref{fig:enc_good_rasp2_packet_gf256}, we observe that the packet size for
the maximum \ac{RLNC} goodput has shifted towards 8 KiB, indicating that
the \ac{Raspi} 2 is able to handle $16 \times 8$ KiB $= 128$ KiB. This is
possible because the \ac{Raspi} 2 has a shared L2 cache of 256 KiB allowing
to still allocate some space for the data to be processed while achieving
a maximum goodputs of 105 MB/s.

\subsubsection{Decoding}
Similar as the encoding goodput, in this Section, we review decoder
goodput in terms of performance and configurations.
Fig.~\ref{fig:dec_good_rasp1} shows the results of the decoder goodput
measurements for the \ac{Raspi} 1 and Fig. \ref{fig:dec_good_rasp2} for the \ac{Raspi} 2.

\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=1.15\textwidth]{images/06_06_2016/goodput_vs_generation_size_Rasp_decoder_Binary_1600.eps}
        \caption[]%
        {{\small Goodput vs. Generation size for $q = 2$}}
        \label{fig:dec_good_rasp1_gen_gf2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=1.15\textwidth]{images/06_06_2016/goodput_vs_generation_size_Rasp_decoder_Binary8_1600.eps}
        \caption[]%
        {{\small Goodput vs. Generation size for $q = 2^8$}}
        \label{fig:dec_good_rasp1_gen_gf256}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=1.15\textwidth]{images/06_06_2016/goodput_vs_symbol_size_Rasp_decoder_Binary_16.eps}
        \caption[]%
        {{\small Goodput vs. Packet size for $q = 2,\ g=16$}}
        \label{fig:dec_good_rasp1_packet_gf2}
    \end{subfigure}
    \quad
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=1.15\textwidth]{images/06_06_2016/goodput_vs_symbol_size_Rasp_decoder_Binary8_16.eps}
        \caption[]%
        {{\small Goodput vs. Packet size for $q = 2^8,\ g=16$}}
        \label{fig:dec_good_rasp1_packet_gf256}
    \end{subfigure}
    \caption[]
    {\small Decoder goodput measurements for the \ac{Raspi} 1}
    \label{fig:dec_good_rasp1}
\end{figure*}

In Fig.~\ref{fig:dec_good_rasp1} and \ref{fig:dec_good_rasp2}, we
observe the same generation and packet size effects reported in the
encoding case. However, we do observe in
Fig.~\ref{fig:dec_good_rasp1_gen_gf2}, \ref{fig:dec_good_rasp1_gen_gf256},
\ref{fig:dec_good_rasp2_gen_gf2} and \ref{fig:dec_good_rasp2_gen_gf256};
that doubling the generation size does not reduce the goodput by a
factor of four. In principle, given that Gaussian elimination scales as
$\mathcal{O}(g^3)$ (and thus the processing time), we would expect that
the goodput to scale as $\mathcal{O}(R_{proc,dec}) = \mathcal{O}(g/g^{3}) =
\mathcal{O}(g^{-2})$. This would imply that doubling the generation
size, should reduce the goodput by a factor of four, which is not the case. Instead,
the goodput is only reduced by a factor of two. This is only possible if
the Gaussian elimination is $\mathcal{O}(g^2)$. A study in
\cite{paramanathan2013leanandmean} for \ac{RLNC} speeds in commercial
devices indicated that this is effectively the case. The reason is that
the $g^2$ scaling factor in the scaling law of the Gaussian elimination
is much
higher than the $g^3$ scaling for $g < 512$. Particularly, this factor
relates to the number of field elements in a packet size as mentioned
in \cite{paramanathan2013leanandmean}.

Another difference with the encoding goodput results in the same figures,
is that even though \ac{TSNC} with $e = 16$ packets provides the fastest
encoding, it does not happen to be the same for the decoding. For this
very sparse scheme, the decoding is affected by the amount of linearly dependent
packets generated by, which leads to a higher delay in some cases
(particularly in $g = [64,\ 128]$). For other generation sizes, the
performance of sparse codes is similar.
%
\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=1.15\textwidth]{images/06_06_2016/goodput_vs_generation_size_Rasp_v2_decoder_Binary_1600.eps}
        \caption[]%
        {{\small Goodput vs. Generation size for $q = 2$}}
        \label{fig:dec_good_rasp2_gen_gf2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=1.15\textwidth]{images/06_06_2016/goodput_vs_generation_size_Rasp_v2_decoder_Binary8_1600.eps}
        \caption[]%
        {{\small Goodput vs. Generation size for $q = 2^8$}}
        \label{fig:dec_good_rasp2_gen_gf256}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=1.15\textwidth]{images/06_06_2016/goodput_vs_symbol_size_Rasp_v2_decoder_Binary_16.eps}
        \caption[]%
        {{\small Goodput vs. Packet size for $q = 2,\ g=16$}}
        \label{fig:dec_good_rasp2_packet_gf2}
    \end{subfigure}
    \quad
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=1.15\textwidth]{images/06_06_2016/goodput_vs_symbol_size_Rasp_v2_decoder_Binary8_16.eps}
        \caption[]%
        {{\small Goodput vs. Packet size for $q = 2^8,\ g=16$}}
        \label{fig:dec_good_rasp2_packet_gf256}
    \end{subfigure}
    \caption[]
    {\small Decoder goodput measurements for the \ac{Raspi} 2}
    \label{fig:dec_good_rasp2}
\end{figure*}
%
\subsection{Average Power}
With the energy measurement setup described in Section
\ref{sec:energy_metrics_methods}, we compute the average power of each
device from their elecric current consumption across time in
Fig.~\ref{fig:current_rasp1} and \ref{fig:current_rasp2}. In each
figure, the eletric current values are classified into three possible
currents: Idle, transition and processing. These values are identified by
our post-processing. Later, the values in red are idle currents, green for
transitions and blue for processing.

For a given configuration of a coding scheme, device type and its parameters,
only electrical current processing values are present. In general, we
associate the current samples from a given configuration with its goodput
through a timestamp. In this way, we relate goodput and power / energy
measurements. Once identified the electric current values, we compute the
averages for each configuration as described in Section
\ref{sec:energy_metrics_methods}. In the presented current samples, we observe
a presence of bursty noise. Nevertheless, by taking the average as described
in \eqref{eq:avg_current}, we remove this contribution in the average
processing current.
%
\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=1.1\textwidth]{images/06_06_2016/current_vs_sec_raspberry_change_gen_from_-10sec_to_500sec.eps}
        \caption[]%
        {{\small Electric current for the \ac{Raspi} 1 model}}
        \label{fig:current_rasp1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=1.1\textwidth]{images/06_06_2016/current_vs_sec_raspberry2_change_gen_from_-10sec_to_500sec.eps}
        \caption[]%
        {{\small Electric current for the \ac{Raspi} 2 model}}
        \label{fig:current_rasp2}
    \end{subfigure}

    \caption[]
    {\small Electric current for each \ac{Raspi} model}
    \label{fig:current_rasp}
\end{figure*}
%
By reviewing the processing current samples in Fig.~\ref{fig:current_rasp1}
and \ref{fig:current_rasp2}, we observe that the average processing current
does not change significantly for each \ac{Raspi} model in all the shown
configurations. Therefore, we approximate the electric current consumption
for the devices and compute the average power as indicated in
\eqref{eq:avg_power}. The results are shown in
Table~\ref{tab:average_power_table}.

\begin{table}[H]
\center
\caption{Average power for the \ac{Raspi} models}
\begin{tabular}{|r|r|r|r|}

\hline
\ac{Raspi} Model & $I_{idle,avg}$ (A)& $I_{proc,avg}$ (A) & $P_{avg}$ (W) \\
\hline
\hline
    \ac{Raspi} 1 &      0.320        &            0.360    & 0.200\\
\hline
    \ac{Raspi} 2 &      0.216        &            0.285    & 0.345\\
\hline
\end{tabular}
\vspace{0.2cm}
\label{tab:average_power_table}
\end{table}
%
From Table~\ref{tab:average_power_table}, we notice that the power expenditure
for both models is almost the same. Thus, the energy behaviour is mostly
dependent on the goodput trends since the power is just a scaling constant.
%
\subsection{Energy per bit}
With power and goodput measurements, we compute the energy per bit of
the previously mentioned cases as described in \eqref{eq:energy_per_bit}.
The trends of the energy per bit are the inverse of the goodput since
both metrics are related by an inverse law. As made with the goodput, we
separate the results descriptions according to the operation carried by the
\ac{Raspi}: Encoding and decoding. Besides this, we differentiate between
the models in our study. The energy per bit consumption removes the dependency
on the amount of packets helping to normalize the results and indicating
energy consumption on a fair basis for all the configurations and coding
operations involved in the study.

\subsubsection{Encoding}
Fig.~\ref{fig:enc_ene_rasp1} and \ref{fig:enc_ene_rasp2} show the encoding
energy per bit measurements for the \ac{Raspi} 1 and 2 models, respectively.
We now proceed to analyze first the \ac{Raspi} 1 case pointing out proper
differences with the \ac{Raspi} 2 when applicable.
%
\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=1.05\textwidth]{images/06_06_2016/energy_per_bit_vs_generation_size_Rasp_Binary_encoder_1600.eps}
        \caption[]%
        {{\small Energy per bit vs. Generation size for $q = 2$}}
        \label{fig:enc_ene_rasp1_gen_gf2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=1.05\textwidth]{images/06_06_2016/energy_per_bit_vs_generation_size_Rasp_Binary8_encoder_1600.eps}
        \caption[]%
        {{\small Energy per bit vs. Generation size for $q = 2^8$}}
        \label{fig:enc_ene_rasp1_gen_gf256}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=1.05\textwidth]{images/06_06_2016/energy_per_bit_vs_symbol_size_Rasp_Binary_encoder_128.eps}
        \caption[]%
        {{\small Energy per bit vs. Packet size for $q=2,\ g=128$}}
        \label{fig:enc_ene_rasp1_packet_gf2}
    \end{subfigure}
    \quad
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=1.05\textwidth]{images/06_06_2016/energy_per_bit_vs_symbol_size_Rasp_Binary8_encoder_128.eps}
        \caption[]%
        {{\small Energy per bit vs. Packet size for $q=2^8,\ g=128$}}
        \label{fig:enc_ene_rasp1_packet_gf256}
    \end{subfigure}
    \caption[]
    {\small Encoder energy measurements for the \ac{Raspi} 1}
    \label{fig:enc_ene_rasp1}
\end{figure*}
%
In Fig.~\ref{fig:enc_ene_rasp1_gen_gf2} and \ref{fig:enc_ene_rasp1_gen_gf256},
we see the dependency of the energy per bit processed on the generation size
for the \ac{Raspi} 1 model. In these type of plots, incrementing the
generation size incurs in more processing time per byte sent which leads to
more processing time per bit sent. For \ac{RLNC}, the energy trends
scales as the processing time scale which is $\mathcal{O}(g)$. For sparse
codes, this trend is scaled by the density, thus for sparse we have
$\mathcal{O}(\ceil{gd})$ which can be appreciated in the same figures. We
do also notice that using $GF(2)$ is energy-wise efficient on the a
per-bit basis since less operations are used to perform the \ac{GF}
arithmetics which reduces the amount of energy spent.

In Fig.~\ref{fig:enc_ene_rasp1_packet_gf2} and
\ref{fig:enc_ene_rasp1_packet_gf256} for the same device. We exhibit the
relationship between the energy per bit processed on the packet size which
is the inverse of the goodput vs. the packet size scaling law. We set
$g = 128$ in this case to observe energy per bit consumption in the regime where the
processing time is considerable. As we notice again in this case,
$GF(2)$ presents as the field with the smallest energy per bit consumption
given that is the one that has least complex operations. The trends
for the energy can be explained as follows: As the packet size increases,
we process more coded bits at the same time, which increases the encoding
speed, until we hit the critical packet size. After this value, we spend
more time queueing data towards the cache besides the processing, which
increases the time spent per processed bit and thus the energy.
%
\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=1.05\textwidth]{images/06_06_2016/energy_per_bit_vs_generation_size_Rasp_v2_Binary_encoder_1600.eps}
        \caption[]%
        {{\small Energy per bit vs. Generation size for $q = 2$}}
        \label{fig:enc_ene_rasp2_gen_gf2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=1.05\textwidth]{images/06_06_2016/energy_per_bit_vs_generation_size_Rasp_v2_Binary8_encoder_1600.eps}
        \caption[]%
        {{\small Energy per bit vs. Generation size for $q = 2^8$}}
        \label{fig:enc_ene_rasp2_gen_gf256}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=1.05\textwidth]{images/06_06_2016/energy_per_bit_vs_symbol_size_Rasp_v2_Binary_encoder_128.eps}
        \caption[]%
        {{\small Energy per bit vs. Packet size for $q=2,\ g=128$}}
        \label{fig:enc_ene_rasp2_packet_gf2}
    \end{subfigure}
    \quad
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=1.05\textwidth]{images/06_06_2016/energy_per_bit_vs_symbol_size_Rasp_v2_Binary8_encoder_128.eps}
        \caption[]%
        {{\small Energy per bit vs. Packet size for $q=2^8,\ g=128$}}
        \label{fig:enc_ene_rasp2_packet_gf256}
    \end{subfigure}
    \caption[]
    {\small Encoder energy measurements for the \ac{Raspi} 2}
    \label{fig:enc_ene_rasp2}
\end{figure*}
%
In Fig.~\ref{fig:enc_ene_rasp2_gen_gf2}, \ref{fig:enc_ene_rasp2_gen_gf256},
\ref{fig:enc_ene_rasp2_packet_gf2} and \ref{fig:enc_ene_rasp2_packet_gf256}
we show the encoding energy per bit consumption for the \ac{Raspi} 2.
We clearly see that the effects discussed for the \ac{Raspi} 1 also apply
as well for the \ac{Raspi} 2. Moreover, given that the average power
is in the same order, but the \ac{Raspi} 2 is a faster device,
the energy costs for the \ac{Raspi} 2 are $2\times$ less than the \ac{Raspi} 1
when referring to variable generation sizes and a fixed packet size.
Furthermore, these costs are 1 order of magnitude less for the \ac{Raspi} 2
with respect to the \ac{Raspi} 1 regarding the case of a fixed generation
size and a variable packet size. This makes the \ac{Raspi} 2 to achieve
a minimum encoding energy consumpiton per bit processed of 0.2 nJ for the
binary field in the mentioned regime.

\subsubsection{Decoding}
In Fig.~\ref{fig:dec_ene_rasp1_gen_gf2}, \ref{fig:dec_ene_rasp1_gen_gf256},
\ref{fig:dec_ene_rasp1_packet_gf2} and \ref{fig:dec_ene_rasp1_packet_gf256}
we show the encoding energy per bit consumption for the \ac{Raspi} 1.
Again, we notice very similar behavior and trends as previously
reviewed for the encoding energy per bit expenditure. In this case,
we focus on performance among the coding schemes since the behavior
and trends were previously explained for the encoding case. Later,
we introduce the decoding energy results for the \ac{Raspi} 2 doing
relevant comparisons with the \ac{Raspi} 1.
%
\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=1.05\textwidth]{images/06_06_2016/energy_per_bit_vs_generation_size_Rasp_Binary_decoder_1600.eps}
        \caption[]%
        {{\small Energy per bit vs. Generation size for $q = 2$}}
        \label{fig:dec_ene_rasp1_gen_gf2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=1.05\textwidth]{images/06_06_2016/energy_per_bit_vs_generation_size_Rasp_Binary8_decoder_1600.eps}
        \caption[]%
        {{\small Energy per bit vs. Generation size for $q = 2^8$}}
        \label{fig:dec_ene_rasp1_gen_gf256}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=1.05\textwidth]{images/06_06_2016/energy_per_bit_vs_symbol_size_Rasp_Binary_decoder_128.eps}
        \caption[]%
        {{\small Energy per bit vs. Packet size for $q=2,\ g=128$}}
        \label{fig:dec_ene_rasp1_packet_gf2}
    \end{subfigure}
    \quad
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=1.05\textwidth]{images/06_06_2016/energy_per_bit_vs_symbol_size_Rasp_Binary8_decoder_128.eps}
        \caption[]%
        {{\small Energy per bit vs. Packet size for $q=2^8,\ g=128$}}
        \label{fig:dec_ene_rasp1_packet_gf256}
    \end{subfigure}
    \caption[]
    {\small Decoder energy measurements for the \ac{Raspi} 1}
    \label{fig:dec_ene_rasp1}
\end{figure*}
%
Some differences occurs due to nature of decoding. In this situation,
the reception of linearly dependent coded packets just increases the decoding
delay, therefore reducing the performance of some coding schemes
in terms of the energy per bit consumption. For example, we notice
that using \ac{SRLNC} with $d = 0.02$ outperforms \ac{TSNC} with
$C = 0$ and $e = 16$, for most of the cases in the variable generation
size curves and in all the cases of variable packet size curves of
Fig.~\ref{fig:dec_ene_rasp1}. This is a clear scenario where the decoding
delay is energy-wise susceptible to the transmissions of linearly dependent
coded packets. With the \ac{Raspi} 1, decoding energies per processed
bit of 2 nJ or similar are possible.
%
\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=1.05\textwidth]{images/06_06_2016/energy_per_bit_vs_generation_size_Rasp_v2_Binary_decoder_1600.eps}
        \caption[]%
        {{\small Energy per bit vs. Generation size for $q = 2$}}
        \label{fig:dec_ene_rasp2_gen_gf2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=1.05\textwidth]{images/06_06_2016/energy_per_bit_vs_generation_size_Rasp_v2_Binary8_decoder_1600.eps}
        \caption[]%
        {{\small Energy per bit vs. Generation size for $q = 2^8$}}
        \label{fig:dec_ene_rasp2_gen_gf256}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=1.05\textwidth]{images/06_06_2016/energy_per_bit_vs_symbol_size_Rasp_v2_Binary_decoder_128.eps}
        \caption[]%
        {{\small Energy per bit vs. Packet size for $q=2,\ g=128$}}
        \label{fig:dec_ene_rasp2_packet_gf2}
    \end{subfigure}
    \quad
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=1.05\textwidth]{images/06_06_2016/energy_per_bit_vs_symbol_size_Rasp_v2_Binary8_decoder_128.eps}
        \caption[]%
        {{\small Energy per bit vs. Packet size for $q=2^8,\ g=128$}}
        \label{fig:dec_ene_rasp2_packet_gf256}
    \end{subfigure}
    \caption[]
    {\small Decoder energy measurements for the \ac{Raspi} 2}
    \label{fig:dec_ene_rasp2}
\end{figure*}
%
Finally, in Fig.~\ref{fig:dec_ene_rasp2_gen_gf2},
\ref{fig:dec_ene_rasp2_gen_gf256}, \ref{fig:dec_ene_rasp2_packet_gf2}
and \ref{fig:dec_ene_rasp2_packet_gf256}; we introduce the decoding energy
per bit consumption for the \ac{Raspi} 2. Here, we obtain a reduction
of an order of magnitude in energy per processed bit due to the speed
of the \ac{Raspi} 2. For example, it can be seen that for the binary field
with $g = 16$ packets, we achieve a decoding energy consumption per bit
processed close to 0.1 nJ in practical systems.
%
\subsection{Multicore Network Coding}
\label{subs:multicore-network-coding}

To review the performance of \ac{NC} in a multicore architecture,
%using the \ac{Raspi}
we implemented the algorithm described in
Section~\ref{sub:implementation-multicore} on the \ac{Raspi} 2 model B, which
features four ARMv7 cores in a Broadcom BCM2836 \ac{SOC} with a 900 MHz clock.
%\fxnote{... which features an ARMv7 CPU with four cores in a Broadcom ...}
Each core has a 32 KiB L1 data cache and 32 KiB L1 instruction cache. The cores
share a 512 KiB L2 cache. All the measured results, including the baseline
results, were obtained with
NEON enabled.
%NEON-enabled code adapted from the Fifi library \cite{fifi}.
The \ac{Raspi} 2 has a NEON extension instruction set which
provides an 128-bit \ac{SIMD} instructions that speed the computations.
Fig.~\ref{enc_dec1024}, \ref{enc_dec128} and \ref{enc_dec16} show the
encoding and decoding goodput in MB per second for different generation sizes,
$g = [1024, 128, 16]$ respectively. For $g = [128, 16]$, the displayed results
are the mean values over 1000 measurements, while for $g=1024$, they are mean
values over 100 measurements. The size of each coded packet was fixed to
$1536$~bytes since that is the typical size of an Ethernet frame. The blocked
operations were performed dividing the matrices in squared sub-blocks of $16,\
32\ ,\ 64,\ldots,\ 1024$ operands (words in the Galois field) in height and
width. The figures show only block sizes of $16 \times 16$, and $128 \times 128$
operands since with bigger block sizes, the operands do not fit in cache.
Several testcases are considered and detailed.

\subsubsection{Baseline encoding}
The baseline results involve no recording of the \ac{DAG} and are performed in a
\emph{by-the-book} fashion. The encoder uses only one thread. The difference
between the non-blocked and blocked encoding schemes is that in the blocked
scheme, the matrix multiplications are performed dividing the matrices in
sub-blocks in order to make the algorithm cache efficient as described in
Section~\ref{sub:implementation-multicore}.

\subsubsection{Encoding blocked}
The encoding results are obtained using the method described in
Section~\ref{sub:implementation-multicore}. The time recorded includes the
dependencies resolving, creation of the \ac{DAG}, and the task scheduling. In
practice, it would suffice to calculate and store this information only once per
generation size.

\subsubsection{Decoding blocked}
The differences between encoding and decoding, is that the decoding task
also involves the matrix inversion. Similarly as with the encoding results,
the time recorded includes the dependencies resolving, the creation of the
\ac{DAG} and the task scheduling. However, to decode, these calculations
are also made for inverting the matrix of coding coefficients.

\begin{figure}[ht!]
\centering
% \includegraphics[width=0.7\textwidth]{images/2015-04-18_encoding_decoding_1024.pdf}
%\includegraphics[width=0.7\textwidth]{images/GenSize_1024_SymbolSize_1536.eps}
\includegraphics[width=0.7\textwidth]{images/GenSize_1024_SymbolSize_1536.pdf}
\caption{Encoding and Decoding performance for $g = 1024$. Block size: $128 \times 128$.}
\label{enc_dec1024}
\end{figure}

For $g= 1024$, the blocked baseline measurements outperforms the non blocked
variant. This means that making the matrix multiplication algorithm cache
efficient brings an increase in goodput by a factor of 3.24. When using the
algorithm described in Section~\ref{sub:implementation-multicore}, encoding with
four cores is on average $3.9\times$ faster than with one core. Similarly,
decoding with four codes is $3.9\times$ faster, on average, than decoding with
a single core. Fig.~\ref{enc_dec1024} shows that the implemented algorithm, by
exploiting cache efficiency and only three extra cores provides a $13\times$
gain compared with traditional non-blocked algorithms. With $g = 1024$, the
matrix inversion becomes more expensive than at smaller generations sizes.
Therefore, the decoding goodput is 58\% of the encoding goodput.

\begin{figure}[ht!]
\centering
%\includegraphics[width=0.7\textwidth]{images/GenSize_128_SymbolSize_1536.eps}
\includegraphics[width=0.7\textwidth]{images/GenSize_128_SymbolSize_1536.pdf}
\caption{Encoding and Decoding performance for $g = 128$. Block size: $128 \times 128$}
\label{enc_dec128}
\end{figure}

For $g = 128$, the differences between the baselines operations show that a
blocked algorithm is 8\% faster than the non-blocked variant. Encoding with
four cores is $2.89\times$ faster than with a single core. Due to the smaller
matrix sizes, the gain when using blocked operations in the baselines is not
that significant when compared with $g = 1024$. For the same reason, the matrix
inversion is less expensive. As a consequence, the decoding goodput is 46\%
of the encoding goodput.

\begin{figure}[ht!]
\centering
%\includegraphics[width=0.7\textwidth]{images/GenSize_16_SymbolSize_1536.eps}
\includegraphics[width=0.7\textwidth]{images/GenSize_16_SymbolSize_1536.pdf}
\caption{Encoding and Decoding performance for $g = 16$. Block size: $16 \times 16$}
\label{enc_dec16}
\end{figure}

When $g = 16$, the gains of blocked operations are negligible compared with
the non-blocked ones. The reason behind this behavior is that all the data
fits in the L1 cache. For the scheduled version, since the problem to solve
is so small, the gain when using four cores is a factor of 2.45 compared
with a single core, and 1.46 compared with two cores. Therefore, the
practical benefits in using four cores instead of two are reduced.

The differences in goodput, for all generation sizes, between the
blocked baseline and the single threaded scheduled measurements are due the
time spent resolving the dependencies and the scheduling overhead. These
effects are negligible for big generation sizes, while considerable for
small matrices. For instance, Fig.~\ref{enc_dec16} shows that the encoding
speed when using one core with the described algorithm is 78\% the encoding
speed without the recording and calculation of the \ac{DAG}.

\subsubsection{Comparison of the load of matrix multiplications and inversions}

To compare how much slower is the matrix multiplication with respect to the
matrix inversion for different generation sizes, we ran a set of tests. We used
a single core to perform the operations. We changed the generation sizes,
performed matrices multiplications and matrix inversions, and measured the time
spent doing so which we name $T_{mult}$ and $T_{inv}$. We calculate the ratio
between these two measured times defined as $r = \frac{T_{mult}}{T_{inv}}$.
Table~\ref{runtimes} summarizes the results. The bigger the matrix size, the
smaller is the calculated ratio. This means that when the problems are bigger,
the decoding goodput decreases compared with the encoding goodput.

\begin{table}[H]
\center
\caption{Multiplication and inversion run-times for different generation sizes with 1 thread}
\begin{tabular}{|r|r|r|r|}

\hline
$g$ & $T_{mult}$ (ms) & $T_{inv}$ (ms) &$r$ \\
\hline
\hline

	16   & 1.495     & 0.169    & 8.8 \\
\hline
	32   & 5.365     & 0.514    & 10.4 \\
\hline
	64   & 20.573    & 2.024    & 10.1 \\
\hline
	128  & 81.357    & 11.755   & 6.9 \\
\hline
	256  & 326.587   & 75.451  & 4.3 \\
\hline
	512  & 1354.012  & 540.469  & 2.5 \\
\hline
	1024 & 5965.284  & 4373.329 & 1.3 \\
\hline
\end{tabular}
\vspace{0.2cm}
\label{runtimes}
\end{table}
